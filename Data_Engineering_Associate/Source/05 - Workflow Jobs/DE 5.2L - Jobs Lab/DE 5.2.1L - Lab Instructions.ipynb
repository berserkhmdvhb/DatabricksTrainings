{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd8e3c5-609c-48d5-8135-897a1ac5dac4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64324f89-6d80-4d96-9a5f-d10dced76448",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Lab: Orchestrating Jobs with Databricks\n",
    "\n",
    "In this lab, you'll be configuring a multi-task job comprising of:\n",
    "* A notebook that lands a new batch of data in a storage directory\n",
    "* A Delta Live Table pipeline that processes this data through a series of tables\n",
    "* A notebook that queries the gold table produced by this pipeline as well as various metrics output by DLT\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you should be able to:\n",
    "* Schedule a notebook as a task in a Databricks Job\n",
    "* Schedule a DLT pipeline as a task in a Databricks Job\n",
    "* Configure linear dependencies between tasks using the Databricks Workflows UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5a43a8-9453-4a7e-b1a2-1c7fbbf25c18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nPython interpreter will be restarted.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(1 seconds)\n| completed (1 seconds total)\n\nCreating & using the schema \"hamed_vaheb_jcxq_da_delp_jobs_lab\"...(0 seconds)\nPredefined tables in \"hamed_vaheb_jcxq_da_delp_jobs_lab\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_lab\n| DA.paths.user_db:          dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_lab/database.db\n| DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v01\n| DA.paths.stream_path:      dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_lab/stream\n| DA.paths.storage_location: dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_lab/storage_location\n\nSetup completed (3 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run ../../Includes/Classroom-Setup-05.2.1L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3370284a-8a3b-427c-bcca-0f6c9569593a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Land Initial Data\n",
    "Seed the landing zone with some data before proceeding. \n",
    "\n",
    "You will re-run this command to land additional data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6db695-f9f2-4d72-b6f2-5ed9bb4c5c29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the file 01.json to the dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_lab/stream/01.json\n"
     ]
    }
   ],
   "source": [
    "DA.data_factory.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9f40225-472c-4c42-85b4-36330659506f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schedule a Notebook Job\n",
    "\n",
    "When using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by scheduling a single task.\n",
    "\n",
    "Before we start run the following cell to get the values used in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cb4373-3359-403e-be51-b8e918c53876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
       "        <td><input type=\"text\" value=\"hamed.vaheb-jcxq-da-delp: Jobs Lab\" style=\"width:100%\"></td></tr>\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Batch Notebook Path:</td>\n",
       "        <td><input type=\"text\" value=\"/Users/hamed.vaheb@pwc.lu/data-engineer-learning-path-v1-0-2-notebooks/05 - Workflow Jobs/DE 5.2L - Jobs Lab/DE 5.2.2L - Batch Job\" style=\"width:100%\"></td></tr>\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Query Notebook Path:</td>\n",
       "        <td><input type=\"text\" value=\"/Users/hamed.vaheb@pwc.lu/data-engineer-learning-path-v1-0-2-notebooks/05 - Workflow Jobs/DE 5.2L - Jobs Lab/DE 5.2.4L - Query Results Job\" style=\"width:100%\"></td></tr>\n",
       "        \n",
       "    </table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n        <td><input type=\"text\" value=\"hamed.vaheb-jcxq-da-delp: Jobs Lab\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Batch Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/hamed.vaheb@pwc.lu/data-engineer-learning-path-v1-0-2-notebooks/05 - Workflow Jobs/DE 5.2L - Jobs Lab/DE 5.2.2L - Batch Job\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Query Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/hamed.vaheb@pwc.lu/data-engineer-learning-path-v1-0-2-notebooks/05 - Workflow Jobs/DE 5.2L - Jobs Lab/DE 5.2.4L - Query Results Job\" style=\"width:100%\"></td></tr>\n        \n    </table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.print_job_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80661910-329f-4bc9-9686-4f6e91cf9501",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, we'll start by scheduling the first notebook.\n",
    "\n",
    "Steps:\n",
    "1. Click the **Workflows** button on the sidebar\n",
    "1. Select the **Jobs** tab.\n",
    "1. Click the blue **Create Job** button\n",
    "1. Configure the task:\n",
    "    1. Enter **Batch-Job** for the task name\n",
    "    1. For **Type**, select **Notebook**\n",
    "    1. For **Path**, select the **Batch Notebook Path** value provided in the cell above\n",
    "    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
    "    1. Click **Create**\n",
    "1. In the top-left of the screen, rename the job (not the task) from **`Batch-Job`** (the defaulted value) to the **Job Name** value provided in the cell above.\n",
    "1. Click the blue **Run now** button in the top right to start the job.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all purpose cluster, you will get a warning about how this will be billed as all purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a136add-5534-43df-a47e-e7a927b63f04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schedule a DLT Pipeline as a Task\n",
    "\n",
    "In this step, we'll add a DLT pipeline to execute after the success of the task we configured at the start of this lesson.\n",
    "\n",
    "So that we can focus on Jobs and not Piplines, we are going to use the following utility command to create the pipeline for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9ca961b-ba3a-40cc-8e8c-0b4743475645",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n",
       "        <td><input type=\"text\" value=\"hamed.vaheb-jcxq-da-delp: Pipeline Lab w/Job\" style=\"width:100%\"></td></tr>\n",
       "    \n",
       "    </table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"hamed.vaheb-jcxq-da-delp: Pipeline Lab w/Job\" style=\"width:100%\"></td></tr>\n    \n    </table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.create_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5269d9-a92c-42c8-8398-e3e790013acb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Steps:\n",
    "1. At the top left of your screen, you'll see the **Runs** tab is currently selected; click the **Tasks** tab.\n",
    "1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n",
    "1. Configure the task:\n",
    "    1. Enter **DLT** for the task name\n",
    "    1. For **Type**, select  **Delta Live Tables pipeline**\n",
    "    1. For **Pipeline**, select the pipeline name provided in the cell above<br/>\n",
    "    1. The **Depends on** field defaults to your previously defined task, **Batch-Job** - leave this value as-is\n",
    "    1. Click the blue **Create task** button\n",
    "\n",
    "You should now see a screen with 2 boxes and a downward arrow between them. \n",
    "\n",
    "Your **`Batch-Job`** task will be at the top, leading into your **`DLT`** task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21cc247-6c1d-4b58-ba01-d544fcb67a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schedule an Additional Notebook Task\n",
    "\n",
    "An additional notebook has been provided which queries some of the DLT metrics and the gold table defined in the DLT pipeline. \n",
    "\n",
    "We'll add this as a final task in our job.\n",
    "\n",
    "Steps:\n",
    "1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n",
    "Steps:\n",
    "1. Configure the task:\n",
    "    1. Enter **Query-Results** for the task name\n",
    "    1. For **Type**, select **Notebook**\n",
    "    1. For **Path**, select the **Query Notebook Path** value provided at the start of this lesson\n",
    "    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
    "    1. The **Depends on** field defaults to your previously defined task, **DLT** - leave this value as-is.\n",
    "    1. Click the blue **Create task** button\n",
    "    \n",
    "Click the blue **Run now** button in the top right of the screen to run this job.\n",
    "\n",
    "From the **Runs** tab, you will be able to click on the start time for this run under the **Active runs** section and visually track task progress.\n",
    "\n",
    "Once all your tasks have succeeded, review the contents of each task to confirm expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3cf44bb-9c2a-498e-a62a-a030c7eaa141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.validate_job_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bdd13c6-f370-4950-ad2c-93746a803af5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE 5.2.1L - Lab Instructions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
