{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "617fa70a-6377-4180-8d5b-73ba8afcaaac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e04a3d1-a8a0-4b49-9dfd-92f514537d54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Orchestrating Jobs with Databricks Workflows\n",
    "\n",
    "New updates to the Databricks Jobs UI have added the ability to schedule multiple tasks as part of a job, allowing Databricks Jobs to fully handle orchestration for most production workloads.\n",
    "\n",
    "Here, we'll start by reviewing the steps for scheduling a notebook task as a triggered standalone job, and then add a dependent task using a DLT pipeline. \n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "* Schedule a notebook task in a Databricks Workflow Job\n",
    "* Describe job scheduling options and differences between cluster types\n",
    "* Review Job Runs to track progress and see results\n",
    "* Schedule a DLT pipeline task in a Databricks Workflow Job\n",
    "* Configure linear dependencies between tasks using the Databricks Workflows UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03d318b-0520-4a68-98f8-3ff4f1430451",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| dropping the schema \"hamed_vaheb_jcxq_da_delp_jobs_demo\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_demo\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v01\"\n\nValidating the locally installed datasets:\n| listing local files...(1 seconds)\n| completed (1 seconds total)\n\nCreating & using the schema \"hamed_vaheb_jcxq_da_delp_jobs_demo\"...(1 seconds)\nPredefined tables in \"hamed_vaheb_jcxq_da_delp_jobs_demo\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:      dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_demo\n| DA.paths.user_db:          dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_demo/database.db\n| DA.paths.datasets:         dbfs:/mnt/dbacademy-datasets/data-engineer-learning-path/v01\n| DA.paths.storage_location: dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_demo/storage_location\n| DA.paths.stream_source:    dbfs:/mnt/dbacademy-users/hamed.vaheb@pwc.lu/data-engineer-learning-path/jobs_demo/stream-source\n\nSetup completed (4 seconds)\n"
     ]
    }
   ],
   "source": [
    "%run ../../Includes/Classroom-Setup-05.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16cbea80-f667-40e2-9125-acb08652d81e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schedule a Notebook Job\n",
    "\n",
    "When using the Jobs UI to orchestrate a workload with multiple tasks, you'll always begin by scheduling a single task.\n",
    "\n",
    "Before we start run the following cell to get the values used in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c21ae69-d28a-4466-88db-b1d05e4bbe90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table style=\"width:100%\">\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n",
       "        <td><input type=\"text\" value=\"hamed.vaheb-jcxq-da-delp: Example Job\" style=\"width:100%\"></td></tr>\n",
       "    <tr>\n",
       "        <td style=\"white-space:nowrap; width:1em\">Reset Notebook Path:</td>\n",
       "        <td><input type=\"text\" value=\"/Users/hamed.vaheb@pwc.lu/data-engineer-learning-path-v1-0-2-notebooks/05 - Workflow Jobs/DE 5.1 - Scheduling Tasks with the Jobs UI/DE 5.1.2 - Reset\" style=\"width:100%\"></td></tr>\n",
       "\n",
       "    </table>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n        <td><input type=\"text\" value=\"hamed.vaheb-jcxq-da-delp: Example Job\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Reset Notebook Path:</td>\n        <td><input type=\"text\" value=\"/Users/hamed.vaheb@pwc.lu/data-engineer-learning-path-v1-0-2-notebooks/05 - Workflow Jobs/DE 5.1 - Scheduling Tasks with the Jobs UI/DE 5.1.2 - Reset\" style=\"width:100%\"></td></tr>\n\n    </table>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.print_job_config_v1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e858f2-f461-4c09-8c72-27c3a1637dbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, we'll start by scheduling the next notebook.\n",
    "\n",
    "Steps:\n",
    "1. Click the **Workflows** button on the sidebar.\n",
    "1. Select the **Jobs** tab.\n",
    "1. Click the **Create Job** button.\n",
    "1. Configure the task:\n",
    "    1. Enter **Reset** for the task name\n",
    "    1. For **Type**, select **Notebook**\n",
    "    1. For **Path**, select the **Reset Notebook Path** value provided in the cell above\n",
    "    1. From the **Cluster** dropdown, under **Existing All Purpose Clusters**, select your cluster\n",
    "    1. Click **Create**\n",
    "1. In the top-left of the screen, rename the job (not the task) from **`Reset`** (the defaulted value) to the **Job Name** value provided in the cell above.\n",
    "1. Click the blue **Run now** button in the top right to start the job.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_24.png\"> **Note**: When selecting your all-purpose cluster, you will get a warning about how this will be billed as all-purpose compute. Production jobs should always be scheduled against new job clusters appropriately sized for the workload, as this is billed at a much lower rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839bf568-1987-4a1c-b0c2-805a7bdfc9b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n",
       "\u001B[0;32m<command-1232511262302884>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0;31m \u001B[0mDA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidate_job_v1_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m<command-1232511262302527>\u001B[0m in \u001B[0;36mvalidate_job_v1_config\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     20\u001B[0m         \u001B[0mtasks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msettings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"tasks\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 21\u001B[0;31m         \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"Expected one task, found {len(tasks)}.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m         \u001B[0mnotebook_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"notebook_task\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"notebook_path\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAssertionError\u001B[0m: Expected one task, found 2."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-1232511262302884>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mDA\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalidate_job_v1_config\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-1232511262302527>\u001B[0m in \u001B[0;36mvalidate_job_v1_config\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m         \u001B[0mtasks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msettings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"tasks\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m         \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34mf\"Expected one task, found {len(tasks)}.\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[0mnotebook_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"notebook_task\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"notebook_path\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAssertionError\u001B[0m: Expected one task, found 2.",
       "errorSummary": "<span class='ansi-red-fg'>AssertionError</span>: Expected one task, found 2.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DA.validate_job_v1_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36509d32-15c4-4009-85b2-b755236fdf15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cron Scheduling of Databricks Jobs\n",
    "\n",
    "Note that on the right hand side of the Jobs UI, directly under the **Job Details** section is a section labeled **Schedule**.\n",
    "\n",
    "Click on the **Edit schedule** button to explore scheduling options.\n",
    "\n",
    "Changing the **Schedule type** field from **Manual** to **Scheduled** will bring up a cron scheduling UI.\n",
    "\n",
    "This UI provides extensive options for setting up chronological scheduling of your Jobs. Settings configured with the UI can also be output in cron syntax, which can be edited if custom configuration not available with the UI is needed.\n",
    "\n",
    "At this time, we'll leave our job set to the **Manual (Paused)** scheduling type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e25ef8ef-57b5-4c5c-9fa8-2fcb51a65784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review Run\n",
    "\n",
    "As currently configured, our single notebook provides identical performance to the legacy Databricks Jobs UI, which only allowed a single notebook to be scheduled.\n",
    "\n",
    "To Review the Job Run\n",
    "1. Select the **Runs** tab in the top-left of the screen (you should currently be on the **Tasks** tab)\n",
    "1. Find your job. If **the job is still running**, it will be under the **Active runs** section. If **the job finished running**, it will be under the **Completed runs** section\n",
    "1. Open the Output details by clicking on the timestamp field under the **Start time** column\n",
    "1. If **the job is still running**, you will see the active state of the notebook with a **Status** of **`Pending`** or **`Running`** in the right side panel. If **the job has completed**, you will see the full execution of the notebook with a **Status** of **`Succeeded`** or **`Failed`** in the right side panel\n",
    "  \n",
    "The notebook employs the magic command **`%run`** to call an additional notebook using a relative path. Note that while not covered in this course, <a href=\"https://docs.databricks.com/repos.html#work-with-non-notebook-files-in-a-databricks-repo\" target=\"_blank\">new functionality added to Databricks Repos allows loading Python modules using relative paths</a>.\n",
    "\n",
    "The actual outcome of the scheduled notebook is to reset the environment for our new job and pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df5940cf-5101-46e7-9157-1cb108d142ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Schedule a DLT Pipeline as a Task\n",
    "\n",
    "The pipeline we create here is a simplified version of the one in the previous unit.\n",
    "\n",
    "We will use it as part of a scheduled job in this step.\n",
    "\n",
    "So that we can focus on Jobs and not Pipelines, we are going to use the following utility command to create the pipline for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b118dd-4801-4a33-a3e8-b86e5434e5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.create_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "659aae70-8bec-4947-8ec6-0bcb2c7314f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, we need to add the task to run this pipeline.\n",
    "\n",
    "\n",
    "Steps:\n",
    "1. At the top left of your screen, you'll see the **Runs** tab is currently selected; click the **Tasks** tab.\n",
    "1. Click the large blue circle with a **+** at the center bottom of the screen to add a new task\n",
    "1. Configure the task:\n",
    "    1. Enter **DLT** for the task name\n",
    "    1. For **Type**, select  **Delta Live Tables pipeline**\n",
    "    1. For **Pipeline**, select the DLT pipeline you configured previously in this exercise<br/>\n",
    "    Note: The pipeline will start with **DLT-Job-Demo-61** and will end with your email address.\n",
    "    1. The **Depends on** field defaults to your previously defined task, **Reset** - leave this value as-is.\n",
    "    1. Click the blue **Create task** button\n",
    "\n",
    "You should now see a screen with 2 boxes and a downward arrow between them. \n",
    "\n",
    "Your **`Reset`** task will be at the top, leading into your **`DLT`** task. \n",
    "\n",
    "This visualization represents the dependencies between these tasks.\n",
    "\n",
    "Click **Run now** to execute your job.\n",
    "\n",
    "**NOTE**: You may need to wait a few minutes as infrastructure for your job and pipeline is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "338046e6-2798-4d3b-9d18-8647fea8dd5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.validate_job_v2_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a50c7ea-28ee-4ecf-b2a3-63f1429b4b07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Review Multi-Task Run Results\n",
    "\n",
    "Select the **Runs** tab again and then the most recent run under **Active runs** or **Completed runs** depending on if the job has completed or not.\n",
    "\n",
    "The visualizations for tasks will update in real time to reflect which tasks are actively running, and will change colors if task failures occur. \n",
    "\n",
    "Clicking on a task box will render the scheduled notebook in the UI. \n",
    "\n",
    "You can think of this as just an additional layer of orchestration on top of the previous Databricks Jobs UI, if that helps; note that if you have workloads scheduling jobs with the CLI or REST API, <a href=\"https://docs.databricks.com/dev-tools/api/latest/jobs.html\" target=\"_blank\">the JSON structure used to configure and get results about jobs has seen similar updates to the UI</a>.\n",
    "\n",
    "**NOTE**: At this time, DLT pipelines scheduled as tasks do not directly render results in the Runs GUI; instead, you will be directed back to the DLT Pipeline GUI for the scheduled Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5e2f9c-39aa-49d7-a31e-7c30d5d77dd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE 5.1.1 - Task Orchestration",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
