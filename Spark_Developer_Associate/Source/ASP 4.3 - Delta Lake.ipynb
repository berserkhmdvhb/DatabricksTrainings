{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db353bfb-c0b5-4eae-8c75-adbb8525b5e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7deacc25-ff13-4461-a1d5-8c5e275dd068",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Delta Lake\n",
    "\n",
    "##### Objectives\n",
    "1. Create a Delta Table\n",
    "1. Understand the transaction Log\n",
    "1. Read data from your Delta Table\n",
    "1. Update data in your Delta Table\n",
    "1. Access previous versions of table using time travel\n",
    "1. Vacuum\n",
    "\n",
    "##### Documentation\n",
    "- <a href=\"https://docs.delta.io/latest/quick-start.html#create-a-table\" target=\"_blank\">Delta Table</a> \n",
    "- <a href=\"https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html\" target=\"_blank\">Transaction Log</a> \n",
    "- <a href=\"https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html\" target=\"_blank\">Time Travel</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bab2793-98ca-48a3-ac66-99b031c117c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b81d106e-74f6-4325-b876-9887818980d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create a Delta Table\n",
    "Let's first read the Parquet-format BedBricks events dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44575668-3558-4b5b-90b4-e022f5426141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventsDF = spark.read.parquet(eventsPath)\n",
    "display(eventsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8050bae7-9b6e-4f8d-95d6-448e308a8e95",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write the data in Delta format to the directory given by `deltaPath`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12d6d4e5-a100-4407-94ba-9085bd149c64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaPath = workingDir + \"/delta-events\"\n",
    "eventsDF.write.format(\"delta\").mode(\"overwrite\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c80fb313-3ac3-446c-92f5-01dc539252c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Write the data in Delta format as a managed table in the metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "320574a0-3c80-42c4-82aa-90c5bf658682",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eventsDF.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c656f7fc-c08a-48a8-aed5-563772bf9cfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As with other file formats, Delta supports partitioning your data in storage using the unique values in a specified column (often referred to as \"Hive partitioning\").\n",
    "\n",
    "Let's **overwrite** the Delta dataset in the `deltaPath` directory to partition by state. This can accelerate queries that filter by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5f2d0cc-34d5-4869-9d2d-d4a8aa425b66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "stateEventsDF = eventsDF.withColumn(\"state\", col(\"geo.state\"))\n",
    "\n",
    "stateEventsDF.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"state\").option(\"overwriteSchema\", \"true\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a24a94-d8e3-47ae-8d3b-d852b130e283",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Understand the Transaction Log\n",
    "We can see how Delta stores the different state partitions in separate directories.\n",
    "\n",
    "Additionally, we can also see a directory called `_delta_log`, which is the transaction log.\n",
    "\n",
    "When a Delta Lake dataset is created, its transaction log is automatically created in the `_delta_log` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8b2d086-8452-42c2-b86d-6552fb9d2885",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(deltaPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdccc249-10fc-42fc-b089-05ceaf9e49fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When changes are made to that table, these changes are recorded as ordered, atomic commits in the transaction log.\n",
    "\n",
    "Each commit is written out as a JSON file, starting with 00000000000000000000.json.\n",
    "\n",
    "Additional changes to the table generate subsequent JSON files in ascending numerical order.\n",
    "\n",
    "<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/20408077/87174138-609fe600-c29c-11ea-90cc-84df0c1357f1.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72c65a0-8500-4428-9908-0af04a9aa391",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(deltaPath + \"/_delta_log/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "790ad12e-983e-4cc5-8f22-97640c4bc55e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, let's take a look at a transaction log File.\n",
    "\n",
    "\n",
    "The <a href=\"https://docs.databricks.com/delta/delta-utility.html\" target=\"_blank\">four columns</a> each represent a different part of the very first commit to the Delta Table, creating the table.\n",
    "- The `add` column has statistics about the DataFrame as a whole and individual columns.\n",
    "- The `commitInfo` column has useful information about what the operation was (WRITE or READ) and who executed the operation.\n",
    "- The `metaData` column contains information about the column schema.\n",
    "- The `protocol` version contains information about the minimum Delta version necessary to either write or read to this Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcba83bb-50c2-4ae6-9145-77ae2b60b6da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.json(deltaPath + \"/_delta_log/00000000000000000000.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6275cdd1-9037-4427-81ea-e2db428ee0f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "One key difference between these two transaction logs is the size of the JSON file, this file has 206 rows compared to the previous 7.\n",
    "\n",
    "To understand why, let's take a look at the `commitInfo` column. We can see that in the `operationParameters` section, `partitionBy` has been filled in by the `state` column. Furthermore, if we look at the add section on row 3, we can see that a new section called `partitionValues` has appeared. As we saw above, Delta stores partitions separately in memory, however, it stores information about these partitions in the same transaction log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb055295-012e-4cbe-bd6a-a638bc4cfa3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.json(deltaPath + \"/_delta_log/00000000000000000001.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b61753-e57e-450d-b631-69dca8c2fdf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, let's take a look at the files inside one of the state partitions. The files inside corresponds to the partition commit (file 01) in the _delta_log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152c0639-2d16-4a73-b5a6-5aa261fbbd2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(deltaPath + \"/state=CA/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "162be130-e6d6-455d-83af-57cfe6790d53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from your Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43ac0568-73a6-4c4f-93f3-5061db394d65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee33163-cedd-4d7e-928c-fb93960709f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Update your Delta Table\n",
    "\n",
    "Let's filter for rows where the event takes place on a mobile device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "724e4e2d-2d8e-4661-9e45-66c08f734c6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_update = stateEventsDF.filter(col(\"device\").isin([\"Android\", \"iOS\"]))\n",
    "display(df_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe4ee00-7428-45ae-884b-39d004b4d0d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_update.write.format(\"delta\").mode(\"overwrite\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecc8fa13-a6e2-45a3-80b8-1605a4cd40e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(deltaPath)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee2ed036-9998-44bd-8efc-93a273767095",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's look at the files in the California partition post-update. Remember, the different files in this directory are snapshots of your DataFrame corresponding to different commits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6eff7ec-2c5d-42e3-9ba7-a72a452d8540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(deltaPath + \"/state=CA/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4659aad2-cfd9-4cfe-b081-f3bbaae50e20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Access previous versions of table using Time  Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09484cc-6611-4d11-a0a7-e07758b46c70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Oops, it turns out we actually we need the entire dataset! You can access a previous version of your Delta Table using Time Travel. Use the following two cells to access your version history. Delta Lake will keep a 30 day version history by default, but if necessary, Delta can store a version history for longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97232371-546c-417f-b426-86d4da6a8cf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS train_delta\")\n",
    "spark.sql(f\"CREATE TABLE train_delta USING DELTA LOCATION '{deltaPath}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da27560f-b4f7-410b-a4d9-eea6941eb7d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY train_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e9c8467-ddf1-456d-a56d-d836849d8031",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using the `versionAsOf` option allows you to easily access previous versions of our Delta Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84578425-1dcc-4869-b039-dc77a0f56425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f82e5e-5eba-4a65-ad1f-1422af38fa65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can also access older versions using a timestamp.\n",
    "\n",
    "Replace the timestamp string with the information from your version history. Note that you can use a date without the time information if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b61ac884-f011-40de-b17e-d1e88352bcec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "timeStampString = <FILL_IN>\n",
    "df = spark.read.format(\"delta\").option(\"timestampAsOf\", timeStampString).load(deltaPath)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c21c27c-21b3-40da-8d3a-40d52a83b2cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Vacuum \n",
    "\n",
    "Now that we're happy with our Delta Table, we can clean up our directory using `VACUUM`. Vacuum accepts a retention period in hours as an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af834f07-2f1a-4eda-9291-2be5903f137b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It looks like our code doesn't run! By default, to prevent accidentally vacuuming recent commits, Delta Lake will not let users vacuum a period under 7 days or 168 hours. Once vacuumed, you cannot return to a prior commit through time travel, only your most recent Delta Table will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dac2de1c-0b97-42c2-8e1c-663957cf7349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import *\n",
    "\n",
    "# deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "# deltaTable.vacuum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bc9b54b-1cbc-4dcb-93fd-08c38f0339ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can workaround this by setting a spark configuration that will bypass the default retention period check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ac7df4-fbc8-4221-8e8a-4052f04fb386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.vacuum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a2f8c48-c1fc-45b4-8baa-1a4beb99f5d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at our Delta Table files now. After vacuuming, the directory only holds the partition of our most recent Delta Table commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a16f72-d34f-4bf3-913f-6e7c3c6bdf7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(deltaPath + \"/state=CA/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06700086-dc9e-4b4b-87e0-5c0e7fa84e93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since vacuuming deletes files referenced by the Delta Table, we can no longer access past versions. The code below should throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8548f10d-f5f4-4902-b7b5-fce72191a780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "133a2fa8-81ba-44c7-a1fd-786774fd6d76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Delta Lab\n",
    "##### Tasks\n",
    "1. Write sales data to Delta\n",
    "1. Modify sales data to show item count instead of item array\n",
    "1. Rewrite sales data to same Delta path\n",
    "1. Create table and view version history\n",
    "1. Time travel to read previous version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd7fa9b9-9f8d-4c8e-a463-d472846f0e37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "salesDF = spark.read.parquet(salesPath)\n",
    "deltaSalesPath = workingDir + \"/delta-sales\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ddb709a-2d2a-47c0-b6d7-4ee9f601d26a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Write sales data to Delta\n",
    "Write **`salesDF`** to **`deltaSalesPath`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5504d7-dca5-44f9-a7ef-3aff769488cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "salesDF.FILL_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fb317ed-8d00-4d4c-b7ba-34ec1da46e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5272ebb3-03aa-4052-a8d4-0f5cb3a14ba5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert len(dbutils.fs.ls(deltaSalesPath)) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8472bd9-938f-4704-b2e4-b48157bfca67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Modify sales data to show item count instead of item array\n",
    "Replace values in the **`items`** column with an integer value of the items array size.  \n",
    "Assign the resulting DataFrame to **`updatedSalesDF`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6c36b48-4aad-4f8f-8a10-da7a6e9606ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "updatedSalesDF = FILL_IN\n",
    "display(updatedSalesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05bf9716-8dc5-48a4-b4c4-cc945ec44b89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e73bb389-88e2-412e-bb64-05d73a01ae31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "assert updatedSalesDF.schema[6].dataType == IntegerType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "044a4140-1de3-4615-a965-95ab156e52d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Rewrite sales data to same Delta path\n",
    "Write **`updatedSalesDF`** to the same Delta location **`deltaSalesPath`**.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_hint_32.png\" alt=\"Hint\"> This will fail without an option to overwrite the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9484736c-3772-4f6d-a0c1-37369841b63f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "updatedSalesDF.FILL_IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d77136e-5a7d-4763-9133-b19ae2c78645",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a56a8b8-cb04-461b-95da-fea7089b802f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert spark.read.format(\"delta\").load(deltaSalesPath).schema[6].dataType == IntegerType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c7af26c-2a89-4e29-9c6b-ef88bbf18072",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Create table and view version history\n",
    "Run SQL queries to perform the following steps.\n",
    "- Drop table **`sales_delta`** if it exists\n",
    "- Create **`sales_delta`** table using the **`deltaSalesPath`** location\n",
    "- List version history for the **`sales_delta`** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ef309e5-71b8-4a0a-b7cb-d23572aa111a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed8dfd8b-ebe6-4ddf-9791-d40f6707ef82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe470e42-19e0-4b67-928b-f18a4556f0fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7de427b-f512-4c77-bdcb-fdda25aae08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "salesDeltaDF = spark.sql(\"SELECT * FROM sales_delta\")\n",
    "assert salesDeltaDF.count() == 210370\n",
    "assert salesDeltaDF.schema[6].dataType == IntegerType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a88e2990-4cf9-4d04-9ec8-e81f9ec47221",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Time travel to read previous version\n",
    "Read delta table at **`deltaSalesPath`** at version 0.  \n",
    "Assign the resulting DataFrame to **`oldSalesDF`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49135ab6-4bbb-4937-a43b-02a139fe3e83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "oldSalesDF = FILL_IN\n",
    "display(oldSalesDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98fbeeb8-df8a-4003-8964-98a5e6b13b30",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**CHECK YOUR WORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f32a1978-12b9-4357-b8c3-dbce7494f8d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assert oldSalesDF.select(size(col(\"items\"))).first()[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "493fee5e-9c21-450e-9f18-a43b12c898ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Clean up classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6354fc3e-f42d-47e8-b844-fdac1e6be03c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e4b449-f479-4674-95eb-f68360c1dc03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "-sandbox\n",
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ASP 4.3 - Delta Lake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
